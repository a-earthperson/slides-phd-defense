
\section{Research Objective 4: Convergence Guarantees}
\begin{frame}
    \Large{\centerline{\textbf{Research Objective 4}}}
    \vspace{6pt}
    \large{\centerline{\textbf{Develop Robust Convergence Criteria}}}
\end{frame}

% ------------------------------------
%  Criterion 1 – Frequentist Half-Width
% ------------------------------------
\begin{frame}{Criterion 1: Frequentist Half-Width (Wald)}
  \footnotesize
  Let $\{Y^{(i)}\}_{i=1}^N$ be i.i.d. Bernoulli trials with $P=\Pr[Y=1]$.  The estimator $\widehat P_N=\tfrac1N\sum_i Y^{(i)}$ obeys the CLT:
  \[\sqrt{N}\,\bigl(\widehat P_N-P\bigr) \;\xrightarrow{d}\; \mathcal N\bigl(0, P(1-P)\bigr).\]
  A $(1-\alpha)$ two–sided \textbf{Wald} interval is therefore
  \[ \widehat P_N \;\pm\; z_{1-\alpha/2}\, \sqrt{\tfrac{\widehat P_N(1-\widehat P_N)}{N}}. \]
  \begin{alertblock}{Stopping rule}
   \textbf{Half-width:} $h_N^{\text{lin}}(z)=z\sqrt{\widehat P_N(1-\widehat P_N)/N}.$ Declare convergence when $h_N^{\text{lin}}/\widehat P_N\le \varepsilon_{\text{rel}}$.
  \end{alertblock}
  \begin{block}{Intuition}
    Shrinks a \emph{relative} confidence band around the estimate; fast for moderate $P$, slow for rare events where $\widehat P_N\ll10^{-4}$.
  \end{block}
\end{frame}

% ------------------------------------
%  Criterion 2 – Bayesian Credible Interval
% ------------------------------------
\begin{frame}{Criterion 2: Bayesian Credible Interval (Jeffreys prior)}
  \footnotesize
  Prior $p\sim\text{Beta}\bigl(\tfrac12,\tfrac12\bigr)$ is invariant under re-parameterization.  After $s$ successes and $f$ failures the posterior is
  \[ p\mid\text{data}\;\sim\; \text{Beta}\bigl(s+\tfrac12,\,f+\tfrac12\bigr). \]
  The central $(1-\alpha)$ credible interval $[q_t,q_{1-t}]$ with $t=\alpha/2$ has
  \begin{alertblock}{Stopping rule}
  \textbf{half-width} $h_N^{\text{Bayes}}=(q_{1-t}-q_t)/2$. Convergence when $h_N^{\text{Bayes}}/\widehat P_N\le \varepsilon_{\text{rel}}^{\text{Bayes}}$.
  \end{alertblock}
  \begin{block}{Intuition}
  Integrates parameter uncertainty; maintains correct coverage even when $\widehat P_N$ is based on only a handful of observed failures (rare-event tails).
  \end{block}
\end{frame}

% ------------------------------------
%  Criterion 3 – Information Gain
% ------------------------------------
\begin{frame}{Criterion 3: Information-Theoretic Gain}
  \small
  Posterior entropy of $\text{Beta}(\alpha,\beta)$ is
  \[ H(\alpha,\beta)=\ln B(\alpha,\beta)- (\alpha-1)\psi(\alpha)- (\beta-1)\psi(\beta)+ (\alpha+\beta-2)\psi(\alpha+\beta). \]
  After a batch $(\Delta s,\Delta f)$ the \textbf{information gain} is
  \[ I_{\text{batch}} = H(\alpha,\beta) - H(\alpha+\Delta s,\,\beta+\Delta f). \]
  \begin{alertblock}{Stopping rule}
   Stop when $I_{\text{batch}}<I_{\min}$ bits (default $10^{-4}$).
  \end{alertblock}
  \begin{block}{Intuition}
 Scale-free; halts when each new batch conveys negligible Shannon information, preventing oversampling when $P$ is either very small or very large.
  \end{block}
\end{frame}

\subsection{On-the-Fly Updates}
% ----------------------------------------------------------------------------
%  Composite Convergence Diagnostics
% ----------------------------------------------------------------------------
\begin{frame}{Composite Convergence Criteria}
  \textbf{Step--1: Statistical precision per node $v$}
  \[
     N^{(v)}_{\text{req}} = \max\Bigl( N^{(v)}_{\varepsilon},\; N^{(v)}_{\log},\; N^{(v)}_{\text{Bayes}},\; N^{(v)}_{\text{info}} \Bigr)
  \]
  where
  \begin{align*}
     N^{(v)}_{\varepsilon} &:= \Bigl\lceil \tfrac{z^2\,\widehat P_v(1-\widehat P_v)}{(\varepsilon_{\text{rel}}\widehat P_v)^2}\Bigr\rceil,\\[2pt]
     N^{(v)}_{\log} &:= \Bigl\lceil \tfrac{z^2(1-\widehat P_v)}{(\varepsilon^{\log}\,\ln 10)^2\widehat P_v}\Bigr\rceil,\\[2pt]
     N^{(v)}_{\text{Bayes}} &:= \text{Eq.~(Jeffreys)} ,\\[2pt]
     N^{(v)}_{\text{info}} &:= \bigl\lceil (2\ln 2)I_{\min}^{-1}\bigr\rceil.
  \end{align*}
\end{frame}

% ----------------------------------------------------------------------------
%  Composite Stopping Rulke
% ----------------------------------------------------------------------------
\begin{frame}{Composite Stopping Rule}
\footnotesize
\begin{columns}
  \column{0.55\textwidth}
    \textbf{Step--2: Global precision budget}
  \[
     N_{\text{req}} = \max_{v \in \mathcal V} N^{(v)}_{\text{req}}.
  \]
  \textbf{Step--3: Translate to iteration budget}
  \[
     T_{\varepsilon} = \Bigl\lceil \frac{N_{\text{req}}}{N}\Bigr\rceil, \qquad N = B P \omega.
  \]
  \textbf{Step--4: External user limits}
  \[
    T_{\max}\; (\text{iteration cap}), \qquad \tau_{\max}\; (\text{wall-clock cap}).
  \]
  \textbf{Stopping time}
  \[
     T^{\*} = \min \bigl\{\, T_{\varepsilon},\; T_{\max},\; T_{\tau}\bigr\}, \qquad T_{\tau}=\min\{ t:\;\tau(t)\ge \tau_{\max}\}.
  \]

  \column{0.45\textwidth}
    \begin{block}{Intuition}
      \begin{itemize}
        \item Statistical criteria guarantee \emph{precision}.  $T_{\varepsilon}$ dominates when resources are ample.
        \item Rule is conservative: run halts \emph{as soon as any} limit is met.
      \end{itemize}
    \end{block}
\end{columns}
\end{frame}



% ----------------------------------------------------------------------------
%  On-the-Fly Diagnostics & Accuracy Metrics
% ----------------------------------------------------------------------------
\begin{frame}{On-the-Fly Diagnostics \& Accuracy Metrics}
\footnotesize
Additional diagnostics when \texttt{true P} is known (for debugging).
\centering
\begin{tabular}{l l}
\toprule
\textbf{Metric} & \textbf{Formula}\\
\midrule
Absolute error & $\Delta_v = |\widehat P_v-P_v|$\\[2pt]
Relative error & $\delta_v = \Delta_v / P_v$\\[2pt]
Mean-squared error & $\text{MSE}_v =(\widehat P_v-P_v)^2$\\[2pt]
Linear half-width & $h^{\text{lin}}_v = z\sqrt{\widehat P_v(1-\widehat P_v)/N}$\\[2pt]
Log half-width & $h^{\log}_v = \dfrac{z\widehat \sigma_v}{\widehat P_v\ln 10}$\\[2pt]
Bayesian half-width & $h^{\text{Bayes}}_v = \dfrac{q_{1-t}-q_t}{2}$\\[2pt]
Information gain & $I_{\text{batch}} = H(\alpha,\beta)-H(\alpha+\Delta s,\,\beta+\Delta f)$\\[2pt]
z--score & $z_v = \dfrac{\widehat P_v-P_v}{\widehat \sigma_v}$\\[2pt]
$\chi^{2}$ goodness-of-fit & $\chi^{2}_v = \frac{(O_1-E_1)^2}{E_1}+\frac{(O_0-E_0)^2}{E_0}$\\
\bottomrule
\end{tabular}
\end{frame}
